{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Reinforcement learning - <b>Naughts and Crosses</b></font>\n",
    "\n",
    "Welcome! Here is my attempt to use Reinforcement learning for <b>naughts and crosses</b> problem. Of course this problem could be decided recursively using <b>Minimax</b> but this game is taken for understanding for applying the same ideas for chess or checkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I will introduce Minimax algorithm just to make a strong opponent for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimax function algorithm\n",
    "# position is a vector of board, where \n",
    "# - 0 - empty square\n",
    "# - 1 - X\n",
    "# - -1 - O\n",
    "\n",
    "def calc_position(position):\n",
    "    # condition of winning - 3 in a row (8 possible cases)\n",
    "    if (position[0] == position[1] == position[2] != 0) or (position[3] == position[4] == position[5] != 0) or \\\n",
    "        (position[6] == position[7] == position[8] != 0) or (position[0] == position[3] == position[6] != 0) or \\\n",
    "        (position[1] == position[4] == position[7] != 0) or (position[2] == position[5] == position[8] != 0) or \\\n",
    "        (position[0] == position[4] == position[8] != 0) or (position[2] == position[4] == position[6] != 0):\n",
    "        all_positions[tuple(position)]=[[],[],sum(position)*2-1]\n",
    "        return None, sum(position)*2-1 # return -1 if O wins and 1 if X wins\n",
    "    \n",
    "    if 0 not in position:\n",
    "        all_positions[tuple(position)]=[[],[],0]\n",
    "        return None, 0 # return 0 as draw\n",
    "    \n",
    "    # turn to move\n",
    "    turn=1\n",
    "    if sum(position)!=0: # if sum of positions not equals 0 - then move of O\n",
    "        turn=-1\n",
    "    \n",
    "    # save moves and values of the positions afrer each move\n",
    "    moves=[]\n",
    "    values=[]\n",
    "    for i in range(9):\n",
    "        if position[i]==0:\n",
    "            sub_position=position[:]\n",
    "            sub_position[i]=turn\n",
    "            moves.append(i)\n",
    "            _,value = calc_position(sub_position)\n",
    "            values.append(value)\n",
    "            \n",
    "    if turn==1: \n",
    "        best_value=max(values)\n",
    "        index=np.argmax(values)\n",
    "    else:\n",
    "        best_value=min(values)\n",
    "        index=np.argmin(values)\n",
    "        \n",
    "    good_moves=[] # Save bad and good moves to lists\n",
    "    bad_moves=[]\n",
    "    for i,value in enumerate (values):\n",
    "        if value==best_value:\n",
    "            good_moves.append(moves[i])\n",
    "        else:\n",
    "            bad_moves.append(moves[i])\n",
    "    all_positions[tuple(position)]=[good_moves,bad_moves,values[index]]\n",
    "    return good_moves,values[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8], 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will try to play from the beginning position\n",
    "all_positions = {}\n",
    "\n",
    "position = [0,0,0,\n",
    "            0,0,0,\n",
    "            0,0,0]\n",
    "\n",
    "calc_position(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see if <b>X</b> and <b>O</b> will play best way the result will be draw for every of the first possible moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 6, 8], 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After the first move of X in the center\n",
    "position = [0,0,0,\n",
    "            0,1,0,\n",
    "            0,0,0]\n",
    "\n",
    "calc_position(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the position below <b>O</b> has only 4 moves of 8 to make a draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start teaching! But first it's better to save all possible positions with bag and good moves to dictionary because if will take few seconds to compute the best move for each game.\n",
    "\n",
    "For this case we can use the same <b>calc_position()</b> function just uncomment all lines with <b>all_positions[tuple(position)]=...</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8], 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positions={}\n",
    "calc_position([0,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2, 3, 5, 6, 8], [7], 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positions[(0,-1,0,0,1,0,0,0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary <b>all_positions</b> contains tuples of all possible positions as keys and as values the list of best moves, the list of other moves and a result after the best play for both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model as Tensorflow graph\n",
    "\n",
    "# Weights and biases of multilayer perceptron\n",
    "W1 = tf.Variable(tf.random.truncated_normal([9, 30], stddev=0.1))\n",
    "W2 = tf.Variable(tf.random.truncated_normal([30, 10], stddev=0.1))\n",
    "W3 = tf.Variable(tf.random.truncated_normal([10, 1], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1, shape=[30]))\n",
    "B2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "B3 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "\n",
    "#x  = tf.placeholder(tf.float32, [None, 9], name='x') # input\n",
    "#y_ = tf.placeholder(tf.float32, [None, 1],  name='y_') # label\n",
    "\n",
    "#g  = tf.placeholder(tf.float32, [None], name='gamma') # reward function coefficients\n",
    "\n",
    "# model prediction for current move\n",
    "def forward(x,W1,W2,W3,B1,B2,B3,y_,g):\n",
    "    res = tf.nn.tanh(tf.matmul(x,W1)+B1)\n",
    "    res = tf.nn.tanh(tf.matmul(res,W2)+B2)\n",
    "    res = tf.nn.tanh(tf.matmul(res,W3)+B3)\n",
    "    # reward function for a set of moves\n",
    "    y = tf.reduce_sum(res*g)\n",
    "    # mse loss function\n",
    "    loss = (y_-y)**2\n",
    "    return loss,res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teaching the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:21<00:00, 26.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# gamma coefficients of the reward function\n",
    "k=0.8\n",
    "gamma=np.zeros(5)\n",
    "for i in range(len(gamma)):\n",
    "    gamma[i]=np.power(k,i+1)\n",
    "mistake_rate = 0.1 # It will be good to let the compluter to lose sometimes\n",
    "alpha = 0.001\n",
    "epochs = 10000\n",
    "\n",
    "for games in tqdm(range(epochs)):\n",
    "    # Now computer plays for O\n",
    "    \n",
    "    current_position=list(np.zeros([9])) # Starting position is always the same\n",
    "    x=[] # input matrix\n",
    "    y=np.zeros([1,1]) # label\n",
    "    \n",
    "    while True:\n",
    "        good_moves,bad_moves,result=all_positions[tuple(current_position)]\n",
    "        \n",
    "        if len(good_moves)+len(bad_moves)==0: # if no moves then the game is finished\n",
    "            y = result\n",
    "            break\n",
    "            \n",
    "        positions=[] # we need to predict the value of possible moves\n",
    "        for move in good_moves:\n",
    "            positions.append(current_position[:])\n",
    "            positions[-1][move]=1\n",
    "        for move in bad_moves:\n",
    "            positions.append(current_position[:])\n",
    "            positions[-1][move]=1\n",
    "          \n",
    "        positions = np.array(positions, dtype=np.float32)\n",
    "        loss, r = forward(positions,W1,W2,W3,B1,B2,B3,y,gamma[:len(positions)])\n",
    "        r = np.array(r)\n",
    "        r = (r+1)/sum(r+1) # Normalizing predictions\n",
    "        \n",
    "        move = np.random.choice(good_moves+bad_moves,p=r[:,0]) # Choose the move by random choice\n",
    "        current_position[move] = 1\n",
    "        x.append(current_position[:])\n",
    "        \n",
    "        # Now we generate the reply of the computer\n",
    "            \n",
    "        good_moves,bad_moves,result=all_positions[tuple(current_position)]\n",
    "        \n",
    "        if len(good_moves)+len(bad_moves)==0:\n",
    "            y = result\n",
    "            break\n",
    "        \n",
    "        if len(bad_moves)>0 and np.random.rand()<=mistake_rate: # According to the mistake_rate make a mistake\n",
    "            move = np.random.choice(bad_moves)\n",
    "        else:\n",
    "            move = np.random.choice(good_moves)\n",
    "        \n",
    "        current_position[move] = -1\n",
    "    \n",
    "    # Train\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "\n",
    "    for j in range(10):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch([W1,W2,W3,B1,B2,B3])\n",
    "            loss, r = forward(x,W1,W2,W3,B1,B2,B3,y,gamma[0:len(x)])\n",
    "            #print(loss)\n",
    "\n",
    "        dJ = g.gradient(loss, [W1,W2,W3,B1,B2,B3])\n",
    "        W1 = W1 - alpha*dJ[0]\n",
    "        W2 = W2 - alpha*dJ[1]\n",
    "        W3 = W3 - alpha*dJ[2]\n",
    "        B1 = B1 - alpha*dJ[3]\n",
    "        B2 = B2 - alpha*dJ[4]\n",
    "        B3 = B3 - alpha*dJ[5]\n",
    "\n",
    "# Now computer plays for X. The same code but without predictions\n",
    "\n",
    "    current_position=list(np.zeros([9]))\n",
    "    x=[]\n",
    "    y=np.zeros([1,1])\n",
    "\n",
    "    while True:\n",
    "        good_moves,bad_moves,result=all_positions[tuple(current_position)]\n",
    "        \n",
    "        if len(good_moves)+len(bad_moves)==0:\n",
    "            y = result\n",
    "            break\n",
    "        \n",
    "        if len(bad_moves)>0 and np.random.rand()<=mistake_rate:\n",
    "            move = np.random.choice(bad_moves)\n",
    "        else:\n",
    "            move = np.random.choice(good_moves)\n",
    "        \n",
    "        current_position[move] = 1\n",
    "        x.append(current_position[:])\n",
    "        \n",
    "        good_moves,bad_moves,result=all_positions[tuple(current_position)]\n",
    "        \n",
    "        if len(good_moves)+len(bad_moves)==0:\n",
    "            y = result\n",
    "            break\n",
    "        \n",
    "        if len(bad_moves)>0 and np.random.rand()<=mistake_rate:\n",
    "            move = np.random.choice(bad_moves)\n",
    "        else:\n",
    "            move = np.random.choice(good_moves)\n",
    "        \n",
    "        current_position[move] = -1\n",
    "        \n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    \n",
    "    for j in range(10):\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch([W1,W2,W3,B1,B2,B3])\n",
    "            loss, r = forward(x,W1,W2,W3,B1,B2,B3,y,gamma[0:len(x)])\n",
    "            #print(loss)\n",
    "\n",
    "        dJ = g.gradient(loss, [W1,W2,W3,B1,B2,B3])\n",
    "        W1 = W1 - alpha*dJ[0]\n",
    "        W2 = W2 - alpha*dJ[1]\n",
    "        W3 = W3 - alpha*dJ[2]\n",
    "        B1 = B1 - alpha*dJ[3]\n",
    "        B2 = B2 - alpha*dJ[4]\n",
    "        B3 = B3 - alpha*dJ[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test our model. For that we'll generate 1000 games and will see what will happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 699.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1000 games with best play of black\n",
    "\n",
    "draws=0\n",
    "loses=0\n",
    "wins=0\n",
    "mistake_rate=0.1 # mistake rate for the computer\n",
    "had_win=0 # count how many winning positions the neural network had\n",
    "had_lose=0 # count how many lost positions the neural network had\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    current_position=list(np.zeros([9]))\n",
    "    had_win_t=0\n",
    "    had_lose_t=0\n",
    "    \n",
    "    while True:\n",
    "        good_moves,bad_moves,result=all_positions[tuple(current_position)]\n",
    "        \n",
    "        if result==-1:\n",
    "            had_lose_t=1\n",
    "        if result==1:\n",
    "            had_win_t=1\n",
    "            \n",
    "        if len(good_moves)+len(bad_moves)==0:\n",
    "            if result==0:\n",
    "                draws+=1\n",
    "            if result==-1:\n",
    "                loses+=1\n",
    "            break\n",
    "            \n",
    "        positions=[]\n",
    "        for move in good_moves:\n",
    "            positions.append(current_position[:])\n",
    "            positions[-1][move]=1\n",
    "        for move in bad_moves:\n",
    "            positions.append(current_position[:])\n",
    "            positions[-1][move]=1\n",
    "            \n",
    "        positions = np.array(positions, dtype=np.float32)\n",
    "        loss, r = forward(positions,W1,W2,W3,B1,B2,B3,y,gamma[:len(positions)])\n",
    "        r = np.array(r)\n",
    "        r = (r+1)/sum(r+1) # Normalizing predictions\n",
    "        \n",
    "        move=(good_moves+bad_moves)[np.argmax(r)]\n",
    "        current_position[move]=1\n",
    "             \n",
    "        good_moves,bad_moves,result=all_positions[tuple(current_position)]\n",
    "        \n",
    "        if result==-1:\n",
    "            had_lose_t=1\n",
    "        if result==1:\n",
    "            had_win_t=1\n",
    "            \n",
    "        if len(good_moves)+len(bad_moves)==0:\n",
    "            if result==0:\n",
    "                draws+=1\n",
    "            if result==1:\n",
    "                wins+=1\n",
    "            break\n",
    "        \n",
    "        if len(bad_moves)>0 and np.random.rand()<mistake_rate:\n",
    "            move=np.random.choice(bad_moves)\n",
    "        else:\n",
    "            move=np.random.choice(good_moves)\n",
    "        \n",
    "        current_position[move]=-1\n",
    "            \n",
    "    had_win+=had_win_t\n",
    "    had_lose+=had_lose_t  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games won: 244\n",
      "Games drawn: 748\n",
      "Games lost: 8\n",
      "Had winning position: 315\n",
      "Had lost position: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Games won:\", wins)\n",
    "print(\"Games drawn:\", draws)\n",
    "print(\"Games lost:\", loses)\n",
    "print(\"Had winning position:\", had_win)\n",
    "print(\"Had lost position:\", had_lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
